{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install auto-gptq torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model.\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset for calibration.\n",
    "# Choosing something that is as close as possible to your target use case is best.\n",
    "# Here, we selected a generic chat dataset.\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "\n",
    "# Best practice is to use 4096, but 2048 can be good enough.\n",
    "MAX_SEQ_LEN = 4096\n",
    "\n",
    "# 1024 samples is usually good enough, moving to 2048 can help sometimes.\n",
    "# Note: the bigger this is, the more memory and the longer the runtime.\n",
    "NUM_SAMPLES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 2048/2048 [00:00<00:00, 4677.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pre-process your dataset.\n",
    "# Its a good idea to use the chat template.\n",
    "def preprocess(example):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        example[\"messages\"], tokenize=False,\n",
    "    )}\n",
    "\n",
    "dataset = load_dataset(DATASET_ID, split=\"train_sft\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "ds = dataset.shuffle().select(range(NUM_SAMPLES))\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# BE CAREFUL WITH THE TOKENIZER\n",
    "#   apply_chat_template already adds the bos_token\n",
    "#   so we set add_special_token to false\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=False, max_length=MAX_SEQ_LEN, truncation=True, add_special_tokens=False\n",
    "    ) for example in ds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you describe the top open-world games and how they differ from linear games?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I can describe the top open-world games and how they differ from linear games. \n",
      "\n",
      "open-world games allow players to explore a vast, interconnected world, complete with various side quests, activities, and non-linear storylines. These games are designed to provide a sense of freedom and allow players to make meaningful choices that impact the gameplay experience. Some of the most popular open-world games include:\n",
      "\n",
      "1. The witcher 3: wild hunt - a fantasy rpg where players control geralt of rivia, a monster hunter who travels a vast open world filled with diverse environments, characters, and side quests.\n",
      "\n",
      "2. Grand theft auto v - an action-adventure game where players control multiple protagonists who explore a fictionalized version of los angeles, engaging in various criminal activities, and driving around in a wide range of vehicles.\n",
      "\n",
      "3. Red dead redemption 2 - an open-world western game where players control arthur morgan, a member of a notorious outlaw gang, and explore a vast, diverse world filled with wildlife, locations, and missions.\n",
      "\n",
      "on the other hand, linear games offer a more controlled, streamlined experience that follows a predetermined storyline. Players follow a set path, with little room for exploration or deviation. Some of the most popular linear games include:\n",
      "\n",
      "1. The last of us 2 - a post-apocalyptic game where players follow the story of ellie and joel, survivors of a fungal outbreak that has turned the majority of the human population into zombie-like creatures.\n",
      "\n",
      "2. Call of duty: modern warfare - an fps game that is part of a popular series that follows a military storyline, where players follow a predetermined path and try to complete missions as quickly and effectively as possible.\n",
      "\n",
      "the key difference between open-world and linear games lies in the level of freedom and exploration that players have. Open-world games provide a sense of freedom and allow players to make choices that impact the gameplay experience, while linear games offer little room for deviation and provide a more controlled, streamlined experience.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I love open-world games because I get to explore and discover things on my own. Have you played any of the games you mentioned?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I do not have the ability to play games or have personal experiences. However, I can tell you that the witcher 3: wild hunt, grand theft auto v, and red dead redemption 2 are some of the most popular open-world games that offer vast and immersive worlds to explore. Each game has its own unique features, such as the fantasy setting and magical combat in the witcher 3, the criminal underworld and high-speed vehicles of grand theft auto v, and the wild west adventure and interesting characters in red dead redemption 2. These games offer endless exploration and provide an immersive experience that can keep players engaged for hours.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Yeah, I totally agree. I love getting lost in exploring the world and discovering hidden gems. Have you heard of any upcoming open-world games that I should keep an eye on?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, there are several highly anticipated open-world games that you might be interested in keeping an eye on. Here are some of them:\n",
      "\n",
      "1. Horizon Forbidden West - This game is a sequel to Horizon Zero Dawn and takes place in a post-apocalyptic world where the player controls Aloy, a skilled hunter who sets out to explore various locations in search of answers to the mysteries of her world.\n",
      "\n",
      "2. Elden Ring - This game is a collaboration between FromSoftware and George R. R. Martin, the creator of Game of Thrones. It promises to be an epic, dark fantasy action game with an open world setting that players will be able to explore.\n",
      "\n",
      "3. Far Cry 6 - The latest installment in the Far Cry series is set in Yara, a fictional Caribbean island that is controlled by a tyrannical dictator. Players control a rebel fighter and explore various locations while fighting to overthrow the regime.\n",
      "\n",
      "4. Cyberpunk 2077 - Set in the futuristic city of Night City, this game promises to be a massive open-world game that allows players to explore and interact with a world populated by cybernetically enhanced humans and robots.\n",
      "\n",
      "All of these games offer unique and exciting open-world experiences that are sure to keep players engaged for hours on end.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Wow, those upcoming open-world games all sound amazing! I'm especially excited for Horizon Forbidden West and Elden Ring. Can't wait to explore their worlds and experience their stories.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm glad to hear that you're excited about those upcoming open-world games! Horizon Forbidden West and Elden Ring are both highly anticipated games that promise to deliver immersive experiences with unique settings and storylines. The open-world settings of these games offer ample opportunities to explore, discover hidden secrets, uncover mysteries, and engage in thrilling combat that will keep you entertained for hours. I hope you'll enjoy playing these games as much as you're looking forward to!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I'm also really looking forward to experiencing the music and sound design in those games. Great music can really help to enhance the immersive experience, don't you think?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, I completely agree with you! A great soundtrack can indeed enhance the immersive experience of a game's world, taking players on a journey through an audio-visual landscape. Skillfully crafted music and sound design can help create emotional responses and add depth to the game's narrative and characters. Music has an all-encompassing effect on the experience - it sets the tone for the game, gives it character, and immerses players in the world. It can help to create a mood, establish a sense of time and place, and even foreshadow events. Overall, good music and sound design can contribute significantly to a game's overall quality and replayability.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Yeah, I remember how the soundtrack in Red Dead Redemption 2 really transported me to the Wild West. The music fit the setting so well and added to the overall experience. Can't wait to see what kind of music Elden Ring and Horizon Forbidden West have in store!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, the music in Red Dead Redemption 2 was truly amazing and perfectly captured the mood and setting of the game's Wild West environment. I'm sure that Elden Ring and Horizon Forbidden West will also have exceptional sound design and music that's tailored to the atmosphere and style of their respective worlds. Each game will likely have a unique soundtrack that helps to elevate the entire gameplay experience, from epic orchestral scores to atmospheric ambient tracks. Keep your ears peeled for the game's official soundtracks and prepare to be swept away by the music and soundscapes of Elden Ring and Horizon Forbidden West.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Confirm the input tokens look right.\n",
    "# Note: its important that the chat template be applied.\n",
    "# Note: careful that you don't end up with 2 <|begin_of_text|> tokens!\n",
    "print(tokenizer.decode(examples[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/home/rshaw/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "# Setup quantization arguments.\n",
    "#   We also support speedup from 8 bits.\n",
    "#   With 8 bits, its best to set group_size=-1 (channelwise) / desc_act=False)\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,                         # Only support 4 bit\n",
    "    group_size=128,                 # Group size 128 is typically the best spot for accuracy / performance.\n",
    "    desc_act=True,                  # Act_recordering will help accuracy.\n",
    "    model_file_base_name=\"model\",   # Name of the model.safetensors when we call save_pretrained\n",
    "    true_sequential=False,          # Set to true to reduce memory consumption for slower runtime of .quantize()\n",
    ")\n",
    "\n",
    "# Load model.\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantize_config,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n"
     ]
    }
   ],
   "source": [
    "# Apply the GPTQ algorithm.\n",
    "model.quantize(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - you are using save_pretrained, which will re-direct to save_quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gptq model to Meta-Llama-3-8B-Instruct-GPTQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Meta-Llama-3-8B-Instruct-GPTQ/tokenizer_config.json',\n",
       " 'Meta-Llama-3-8B-Instruct-GPTQ/special_tokens_map.json',\n",
       " 'Meta-Llama-3-8B-Instruct-GPTQ/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptq_save_dir = f\"{MODEL_ID.split('/')[-1]}-GPTQ\"\n",
    "print(f\"Saving gptq model to {gptq_save_dir}\")\n",
    "model.save_pretrained(gptq_save_dir)\n",
    "tokenizer.save_pretrained(gptq_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
