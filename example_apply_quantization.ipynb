{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install auto-gptq torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model.\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset for calibration.\n",
    "# Choosing something that is as close as possible to your target use case is best.\n",
    "# Here, we selected a generic chat dataset.\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "\n",
    "# Best practice is to use 4096, but 2048 can be good enough.\n",
    "MAX_SEQ_LEN = 4096\n",
    "\n",
    "# 1024 samples is usually good enough, moving to 2048 can help sometimes.\n",
    "NUM_SAMPLES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rshaw/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 2038/2038 [00:00<00:00, 4754.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pre-process your dataset.\n",
    "# Its a good idea to use the chat template.\n",
    "def preprocess(example):\n",
    "    return {\"text\": tokenizer.apply_chat_template(\n",
    "        example[\"messages\"], tokenize=False,\n",
    "    )}\n",
    "\n",
    "dataset = load_dataset(DATASET_ID, split=\"train_sft\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "ds = dataset.shuffle().select(range(NUM_SAMPLES))\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# BE CAREFUL WITH THE TOKENIZER\n",
    "#   apply_chat_template already adds the bos_token\n",
    "#   so we set add_special_token to false\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=False, max_length=MAX_SEQ_LEN, truncation=True, add_special_tokens=False\n",
    "    ) for example in ds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Create a modern drama, written in third-person omniscient point of view, that delves into the emotional complexity of a couple's experience in the aftermath of infidelity. Explore themes such as trust, forgiveness, jealousy, and deceit in a way that captivates the audience and leaves them questioning the boundaries of love and loyalty. The drama should be set in a realistic environment and include dynamic, multi-dimensional characters who evolve throughout the course of the story.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "As soon as the secret was out, nothing was quite the same.\n",
      "\n",
      "It started with a gut feeling. Alice couldn't shake the sense that something was off between her and her husband, Simon. He worked late and came home exhausted, often falling asleep as soon as his head hit the pillow. But that wasn't what bothered her. It was the distance between them—the way he seemed to be holding something back, even as he held her in his arms.\n",
      "\n",
      "One night, while Simon had left his phone on the kitchen table while he went to shower after work, Alice's intuition got the best of her. She picked up the phone, scrolling through his messages until she found what she had been searching for—\n",
      "\n",
      "An exchange between Simon and a woman she didn't recognize. Flirty messages that made her cheeks burn with anger and betrayal. She confronted Simon immediately, but he only winced and shuffled his feet, refusing to meet her gaze. He had slept with the woman, he admitted, a one-time thing he regretted immediately afterward.\n",
      "\n",
      "But his infidelity was a poisoning event that had contaminated the very foundation of their relationship. Alice struggled with waves of jealousy, feeling almost haunted by the thought that Simon had found in someone else what she had failed to give him. She started checking his phone every chance she got, sometimes getting up in the middle of night to scroll through his emails.\n",
      "\n",
      "Simon, in turn, felt suffocated by Alice's lack of trust. He had made a mistake, he acknowledged, but he couldn't go back and undo it. He felt like a prisoner, as if every move he made was being scrutinized for signs of further wrongdoing. He knew that he would do anything to fix their relationship, but Alice's constant accusations and anger made him feel as if he were caught in an endless cycle of punishment.\n",
      "\n",
      "As time went on, Alice and Simon tried to rebuild their relationship, attending couples therapy and trying to be more open with one another. Alice tried to forgive Simon and move past his infidelity, but she couldn't shake the feeling that she was never going to be enough for him. Meanwhile, Simon felt more and more like a stranger in his own home, cut off from the woman who had once been his best friend and confidant.\n",
      "\n",
      "The drama escalates as the story progresses, both characters struggling to reconcile their desires with the reality of their damaged relationship. Trust is shattered, and both Alice and Simon find themselves questioning the boundaries of their love and loyalty. As the audience is swept up into the emotional complexity of their story, they are forced to confront the harsh reality that infidelity can have lasting consequences on any relationship– that love isn't always enough to repair what's been broken.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is a great start, but can you add more details about how the couple's friends and family are affected by their infidelity and how they try to give advice or help them?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "As word of Alice and Simon's infidelity spread throughout their circle of friends and family, they were met with a range of reactions. Some of their loved ones were shocked, others were angry, and still, others tried to play the role of peacemaker, hoping to help the couple repair their damaged relationship.\n",
      "\n",
      "Alice's best friend, Karen, was among the angriest of the bunch. She had always been fiercely loyal to Alice, and the thought that Simon could betray her in such a callous way made her blood boil. She was a shoulder for Alice to cry on at first, but as the weeks wore on, even she struggled with the idea of forgiveness. Karen made it clear that she thought Alice was better off without Simon—that he was never going to change, and that she deserved a partner who would treat her with the love and respect she deserved.\n",
      "\n",
      "On the other hand, Simon's brother Tom tried to take a more sympathetic approach. He had been through infidelity in his own relationship, and he knew how hard it was to rebuild trust once it had been broken. He urged Simon to be patient with Alice, to show her through actions and words that he was remorseful for what he had done. At the same time, he told Alice that she needed to try to let go of her anger, to see that Simon was genuinely trying to make things right.\n",
      "\n",
      "However, their family and friends’ advice and attitudes only added to the complexity of Alice and Simon's struggles. As much as they wanted to believe that they could heal their relationship, the betrayal had damaged something fundamental in both of them. Trust, once shattered, took time and effort to repair. And despite the love they still felt for one another, they found themselves questioning whether it was enough to overcome the pain they had inflicted on each other.\n",
      "\n",
      "In the end, Alice and Simon made the difficult decision to separate. They knew that the road ahead would be long and difficult, but they also knew that they needed time apart to heal and process what had happened. Though it was painful for both of them, they realized that this was the only way they could move forward—together or apart.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Could you add more insight into the characters' individual journeys of healing and self-discovery after their separation? What were some of the challenges they faced and how did they overcome them?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "After their separation, Alice and Simon embarked on separate journeys of healing and self-discovery. It was a time of great introspection for both of them, as they tried to come to terms with the pain and loss they had experienced, and to figure out who they were without each other. Alice, in particular, struggled with feelings of abandonment and worthlessness. She had poured so much of herself into her relationship with Simon, and when it had all fallen apart, she felt completely lost. She found herself constantly questioning her own self-worth, wondering what it was about her that had driven Simon into another woman's arms. At first, she coped with her pain by throwing herself into work, but it soon became clear that this was just a form of escapism. It wasn't until she started seeing a therapist that she began to make progress in her healing. The therapist helped her to see that her self-worth wasn't tied to her relationship with Simon—it was something that came from within, something that was unshakeable no matter what happened in life. Once Alice realized this, she was able to start rebuilding her confidence and self-esteem. She started to rediscover the things that had made her happy before she had met Simon—hobbies, friendships, and travel. She learned to rely on herself and to find value in her own company. On the other hand, Simon struggled with feelings of guilt and regret. He knew that his actions had hurt Alice deeply and that he had let her down in a way that could never be fully repaired. He had always seen himself as a loyal and faithful person, and to realize that he had betrayed Alice after all these years was a real blow to his self-esteem. For Simon, therapy wasn't as effective as it was for Alice. Instead, he turned to meditation and yoga as a way of calming his mind and processing his emotions. Through these practices, he was able to confront the darker parts of himself that had led to his infidelity. He learned to accept responsibility for his actions and to forgive himself for his mistakes. He also discovered a new sense of empathy for others and a desire to help those who had experienced similar struggles. Over time, Alice and Simon began to reconnect as friends. They still loved and cared for each other, but they had both realized that their relationship couldn't be what it once was. They could, however, support each other in their individual journeys. Alice had learned that she was worthy of love and respect, and Simon had learned that he was capable of hurting the person he loved most. Together, they had grown and evolved beyond the constraints of their relationship, and had come out the other side stronger, wiser, and more self-aware.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is a great storyline, but I would love to see more about the emotional struggle Alice went through during the separation, especially in regards to letting go of the anger and hurt. Can you expand on that?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Of course! During their separation, Alice struggled with letting go of the anger and hurt caused by Simon's infidelity. She had been deeply wounded by the betrayal, and though she wanted to move on and heal, she found herself struggling with overwhelming emotions of pain and anger. It was a difficult and sometimes confusing process, but Alice knew that she had to face these emotions head-on if she was ever going to find any sense of closure.\n",
      "\n",
      "Alice tried to stay busy and keep her mind occupied as much as possible. She spent time with friends, threw herself into work, and picked up new hobbies. However, she found that even at the most unexpected moments, her emotions would come roaring back to the surface. Sometimes it was something as small as a song on the radio, or a particular smell, that would trigger memories of the pain Simon had caused her. Other times her anger would bubble over, fuelled by thoughts of how easily Simon had betrayed her trust, as if their relationship meant nothing.\n",
      "\n",
      "As tough as things were, Alice refused to give up. She threw herself into therapy, hoping to find some sort of guidance and support. Her therapist encouraged her to let herself feel her emotions, to acknowledge the pain and anger that she was experiencing. It was only by acknowledging these feelings, the therapist explained, that she could truly start to heal.\n",
      "\n",
      "Alice slowly started to let go of the anger and hurt. She began to understand that she couldn't control Simon's actions or the pain he had caused her. But she could control her own emotions, and gradually, Alice came to accept that she wasn't at fault for Simon's infidelity. It was a slow and painful process, but with patience and persistence, Alice was finally able to reach a point where she could let go of the hurt and move on.\n",
      "\n",
      "One of the things that helped Alice in this process was being mindful of the present moment. She started practicing mindfulness and meditation techniques to help her stay grounded in the here and now. By focusing on her breathing, and gently observing her thoughts and emotions, Alice was able to develop a sense of calmness and clarity. Through mindfulness, she was able to come to terms with her pain and anger, and begin to move towards forgiveness.\n",
      "\n",
      "In the end, Alice forged a new relationship with Simon, one based on mutual respect and understanding. Though it wasn't the same as their previous relationship, it was a testament to Alice's resilience and inner strength. She had faced her emotions head-on and emerged stronger, wiser, and more self-aware.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Confirm the input tokens look right.\n",
    "# Note: its important that the chat template be applied.\n",
    "# Note: careful that you don't end up with 2 <|begin_of_text|> tokens!\n",
    "print(tokenizer.decode(examples[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/home/rshaw/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "# Setup quantization arguments.\n",
    "#   We also support speedup from 8 bits.\n",
    "#   With 8 bits, its best to set group_size=-1 (channelwise) / desc_act=False)\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,                         # Only support 4 bit\n",
    "    group_size=128,                 # Group size 128 is typically the best spot for accuracy / performance.\n",
    "    desc_act=True,                  # Act_recordering will help accuracy.\n",
    "    model_file_base_name=\"model\",   # Name of the model.safetensors when we call save_pretrained\n",
    "    true_sequential=False,          # Set to True to reduce memory consumption during quantization at expense of runtime      \n",
    ")\n",
    "\n",
    "# Load model.\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantize_config,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "INFO - Start quantizing layer 2/32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the GPTQ algorithm.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:452\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.quantize\u001b[0;34m(self, examples, batch_size, use_triton, use_cuda_fp16, autotune_warmup_after_quantized, cache_examples_on_gpu)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m layer_input_kwargs[j]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    451\u001b[0m         additional_layer_inputs[k] \u001b[38;5;241m=\u001b[39m nested_move_to_device(v, cur_layer_device)\n\u001b[0;32m--> 452\u001b[0m     \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_layer_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handles:\n\u001b[1;32m    454\u001b[0m     h\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:624\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    621\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    623\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, position_ids)\n\u001b[0;32m--> 624\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:183\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    181\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    182\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m--> 183\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/auto-gptq/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:153\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    149\u001b[0m         cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(x, position_ids)\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cos, sin\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrotate_half\u001b[39m(x):\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the GPTQ algorithm.\n",
    "model.quantize(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - you are using save_pretrained, which will re-direct to save_quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gptq model to Meta-Llama-3-8B-Instruct-gptq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Meta-Llama-3-8B-Instruct-gptq/tokenizer_config.json',\n",
       " 'Meta-Llama-3-8B-Instruct-gptq/special_tokens_map.json',\n",
       " 'Meta-Llama-3-8B-Instruct-gptq/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptq_save_dir = f\"{MODEL_ID.split('/')[-1]}-gptq\"\n",
    "print(f\"Saving gptq model to {gptq_save_dir}\")\n",
    "model.save_pretrained(gptq_save_dir)\n",
    "tokenizer.save_pretrained(gptq_save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
